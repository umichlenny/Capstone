{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  503 of 503 completed\n",
      "\n",
      "2 Failed downloads:\n",
      "['BRK.B']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "['BF.B']: Exception('%ticker%: No price data found, symbol may be delisted (1d 2005-01-01 -> 2023-12-31)')\n",
      "/var/folders/6r/ss5r1v153zs2x7pfxs5lfmsr0000gn/T/ipykernel_16587/2343829486.py:15: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  df = df.stack().reset_index(level=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "tickers = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "tickers = tickers.Symbol.tolist()\n",
    "df = yf.download(tickers, start='2005-01-01', end='2023-12-31')\n",
    "df = df.stack().reset_index(level=1)\n",
    "df = df[df['Ticker'] =='AAPL']\n",
    "df = df.head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\tBatch 1/12, Loss: 1.1918\n",
      "\tBatch 2/12, Loss: 0.0180\n",
      "\tBatch 3/12, Loss: 0.3116\n",
      "\tBatch 4/12, Loss: 0.3923\n",
      "\tBatch 5/12, Loss: 0.0739\n",
      "\tBatch 6/12, Loss: 0.0093\n",
      "\tBatch 7/12, Loss: 0.0444\n",
      "\tBatch 8/12, Loss: 0.0130\n",
      "\tBatch 9/12, Loss: 0.0161\n",
      "\tBatch 10/12, Loss: 0.0214\n",
      "\tBatch 11/12, Loss: 0.0230\n",
      "\tBatch 12/12, Loss: 0.0358\n",
      "Average Loss: 0.1792\n",
      "Epoch 2/15\n",
      "\tBatch 1/12, Loss: 0.0176\n",
      "\tBatch 2/12, Loss: 0.0161\n",
      "\tBatch 3/12, Loss: 0.0137\n",
      "\tBatch 4/12, Loss: 0.0190\n",
      "\tBatch 5/12, Loss: 0.0101\n",
      "\tBatch 6/12, Loss: 0.0070\n",
      "\tBatch 7/12, Loss: 0.0366\n",
      "\tBatch 8/12, Loss: 0.0213\n",
      "\tBatch 9/12, Loss: 0.0196\n",
      "\tBatch 10/12, Loss: 0.0306\n",
      "\tBatch 11/12, Loss: 0.0312\n",
      "\tBatch 12/12, Loss: 0.0155\n",
      "Average Loss: 0.0199\n",
      "Epoch 3/15\n",
      "\tBatch 1/12, Loss: 0.0237\n",
      "\tBatch 2/12, Loss: 0.0163\n",
      "\tBatch 3/12, Loss: 0.0133\n",
      "\tBatch 4/12, Loss: 0.0165\n",
      "\tBatch 5/12, Loss: 0.0318\n",
      "\tBatch 6/12, Loss: 0.0161\n",
      "\tBatch 7/12, Loss: 0.0181\n",
      "\tBatch 8/12, Loss: 0.0206\n",
      "\tBatch 9/12, Loss: 0.0272\n",
      "\tBatch 10/12, Loss: 0.0140\n",
      "\tBatch 11/12, Loss: 0.0160\n",
      "\tBatch 12/12, Loss: 0.0201\n",
      "Average Loss: 0.0195\n",
      "Epoch 4/15\n",
      "\tBatch 1/12, Loss: 0.0229\n",
      "\tBatch 2/12, Loss: 0.0255\n",
      "\tBatch 3/12, Loss: 0.0365\n",
      "\tBatch 4/12, Loss: 0.0140\n",
      "\tBatch 5/12, Loss: 0.0093\n",
      "\tBatch 6/12, Loss: 0.0097\n",
      "\tBatch 7/12, Loss: 0.0115\n",
      "\tBatch 8/12, Loss: 0.0153\n",
      "\tBatch 9/12, Loss: 0.0168\n",
      "\tBatch 10/12, Loss: 0.0189\n",
      "\tBatch 11/12, Loss: 0.0174\n",
      "\tBatch 12/12, Loss: 0.0186\n",
      "Average Loss: 0.0180\n",
      "Epoch 5/15\n",
      "\tBatch 1/12, Loss: 0.0244\n",
      "\tBatch 2/12, Loss: 0.0274\n",
      "\tBatch 3/12, Loss: 0.0185\n",
      "\tBatch 4/12, Loss: 0.0188\n",
      "\tBatch 5/12, Loss: 0.0212\n",
      "\tBatch 6/12, Loss: 0.0099\n",
      "\tBatch 7/12, Loss: 0.0227\n",
      "\tBatch 8/12, Loss: 0.0218\n",
      "\tBatch 9/12, Loss: 0.0113\n",
      "\tBatch 10/12, Loss: 0.0156\n",
      "\tBatch 11/12, Loss: 0.0275\n",
      "\tBatch 12/12, Loss: 0.0387\n",
      "Average Loss: 0.0215\n",
      "Epoch 6/15\n",
      "\tBatch 1/12, Loss: 0.0181\n",
      "\tBatch 2/12, Loss: 0.0130\n",
      "\tBatch 3/12, Loss: 0.0160\n",
      "\tBatch 4/12, Loss: 0.0115\n",
      "\tBatch 5/12, Loss: 0.0217\n",
      "\tBatch 6/12, Loss: 0.0330\n",
      "\tBatch 7/12, Loss: 0.0156\n",
      "\tBatch 8/12, Loss: 0.0193\n",
      "\tBatch 9/12, Loss: 0.0119\n",
      "\tBatch 10/12, Loss: 0.0249\n",
      "\tBatch 11/12, Loss: 0.0288\n",
      "\tBatch 12/12, Loss: 0.0247\n",
      "Average Loss: 0.0199\n",
      "Epoch 7/15\n",
      "\tBatch 1/12, Loss: 0.0128\n"
     ]
    }
   ],
   "source": [
    "# Loading data^\n",
    "data = df\n",
    "# Define Transformer Model^\n",
    "class StockPricePredictionModel(nn.Module):\n",
    "    #num_layers =3^\n",
    "    #hidden_size=128^\n",
    "    #dropout=0.5^\n",
    "    def __init__(self, input_size, output_size, num_layers = 3, hidden_size = 128, dropout = 0.5):\n",
    "        super(StockPricePredictionModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.fc(pooled_output)\n",
    "\n",
    "# Define Stock Price Dataset^\n",
    "class StockPriceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        text = f\"{item['Ticker']} {item['Adj Close']} {item['Volume']} {item['Close']} {item['High']} {item['Low']} {item['Open']}\"  \n",
    "        inputs = self.tokenizer(text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(item['Adj Close'], dtype=torch.float32)\n",
    "        }\n",
    "        \n",
    "# Load dataset and model^\n",
    "# MODEL^\n",
    "# TOKENIZER^\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MODEL = StockPricePredictionModel(input_size=768, output_size=1)\n",
    "\n",
    "# Sort the data by time^\n",
    "# SORTED_DATAA^\n",
    "SORTED_DATAA = data.sort_values(by='Date')\n",
    "\n",
    "# Set the ratio for dataset^\n",
    "# TRAINING_RATIO^\n",
    "# Training dataset 60%\n",
    "# Testing dataset 40%\n",
    "TRAINING_RATIO = 0.6\n",
    "\n",
    "# Indices^\n",
    "# TRAINING_SIZE^\n",
    "# TRAINING_INDICES^\n",
    "# TESTING_INDICES^\n",
    "TRAINING_SIZE = int(TRAINING_RATIO * len(SORTED_DATAA))\n",
    "TRAINING_INDICES = np.arange(TRAINING_SIZE)\n",
    "TESTING_INDICES = np.arange(TRAINING_SIZE, len(SORTED_DATAA))\n",
    "\n",
    "# Spliting the dataset^\n",
    "# TRAINING_DATA^\n",
    "# TESTING_DATA^\n",
    "TRAINING_DATA = SORTED_DATAA.iloc[TRAINING_INDICES]\n",
    "TESTING_DATA = SORTED_DATAA.iloc[TESTING_INDICES]\n",
    "\n",
    "# Training model\n",
    "# DATASSET^\n",
    "DATASSET = StockPriceDataset(TRAINING_DATA, TOKENIZER)\n",
    "# DATALOADER^\n",
    "DATALOADER = DataLoader(DATASSET, batch_size=16, shuffle=True)\n",
    "# OPTIMIZER^\n",
    "OPTIMIZER = torch.optim.Adam(MODEL.parameters(), lr=1e-4)\n",
    "# CRITERION^\n",
    "CRITERION = nn.MSELoss()\n",
    "# TRAIN_LOSS = []\n",
    "TRAIN_LOSS = []\n",
    "# Set the number of epochs^\n",
    "\n",
    "\n",
    "BEST_MODEL_STATE_DICT = None\n",
    "BEST_LOSS = float('inf')\n",
    "\n",
    "# NUM_EPOCHS^\n",
    "NUM_EPOCHS = 15\n",
    "#EPOCH^\n",
    "for EPOCH in range(NUM_EPOCHS):\n",
    "    # Set total loss^\n",
    "    # TOTAL_LOSS^\n",
    "    TOTAL_LOSS = 0.0\n",
    "    print(f\"Epoch {EPOCH+1}/{NUM_EPOCHS}\")\n",
    "    # BATCH_INDEX^\n",
    "    # BATCH^\n",
    "    for BATCH_INDEX, BATCH in enumerate(DATALOADER):\n",
    "        input_ids = BATCH['input_ids']\n",
    "        attention_mask = BATCH['attention_mask']\n",
    "        labels = BATCH['labels']\n",
    "        \n",
    "        OPTIMIZER.zero_grad()\n",
    "        outputs = MODEL(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        \n",
    "        # LOSS\n",
    "        LOSS = CRITERION(outputs.squeeze(1), labels)\n",
    "        LOSS.backward()\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        # Accumalte the loss^\n",
    "        TOTAL_LOSS += LOSS.item()\n",
    "        \n",
    "        print(f\"\\tBatch {BATCH_INDEX+1}/{len(DATALOADER)}, Loss: {LOSS.item():.4f}\")\n",
    "\n",
    "    # Calculate the average loss for each epoch^\n",
    "    # AVERAGE_LOSS^\n",
    "    AVERAGE_LOSS = TOTAL_LOSS / len(DATALOADER)\n",
    "\n",
    "    if AVERAGE_LOSS < BEST_LOSS:\n",
    "        BEST_LOSS = AVERAGE_LOSS\n",
    "        BEST_MODEL_STATE_DICT = MODEL.state_dict()\n",
    "\n",
    "    TRAIN_LOSS.append(AVERAGE_LOSS)\n",
    "    print(f\"Average Loss: {AVERAGE_LOSS:.4f}\")\n",
    "\n",
    "# Plot the training loss curve\n",
    "plt.plot(TRAIN_LOSS, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST_MODEL_STATE_DICT:\n",
    "    MODEL.load_state_dict(BEST_MODEL_STATE_DICT)\n",
    "\n",
    "# Set to evaluation mode^\n",
    "# MODEL^\n",
    "MODEL.eval()\n",
    "# Store the test loss for each epoch^\n",
    "# TEST_LOSSES^\n",
    "TEST_LOSSES = []\n",
    "\n",
    "\n",
    "#PREDICTED_PRICES^\n",
    "#ACTUAL_PRICES^\n",
    "PREDICTED_PRICES = []\n",
    "ACTUAL_PRICES = []\n",
    "\n",
    "\n",
    "#DATE_LABELS^\n",
    "DATE_LABELS = TESTING_DATA.index.tolist()\n",
    "\n",
    "\n",
    "# Evaluate the model^\n",
    "# TEST_DATASET^\n",
    "# TEST_DATALOADER^\n",
    "TEST_DATASET = StockPriceDataset(TESTING_DATA, TOKENIZER)\n",
    "TEST_DATALOADER = DataLoader(TEST_DATASET, batch_size=10, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # BATCH_INDEX^\n",
    "    # BATCH^\n",
    "    for BATCH_INDEX, BATCH in enumerate(TEST_DATALOADER):\n",
    "        # INPUT_IDS^\n",
    "        # ATTENTION_MASK^\n",
    "        INPUT_IDS = BATCH['input_ids']\n",
    "        ATTENTION_MASK = BATCH['attention_mask']\n",
    "        # LABELS^\n",
    "        LABELS = BATCH['labels']\n",
    "        # OUTPUTS^\n",
    "        OUTPUTS = MODEL(input_ids = INPUT_IDS , attention_mask = ATTENTION_MASK)\n",
    "        PREDICTED_PRICES.extend(OUTPUTS.squeeze(1).tolist())\n",
    "        ACTUAL_PRICES.extend(LABELS.tolist())\n",
    "        # LOSS^\n",
    "        LOSS = CRITERION(OUTPUTS.squeeze(1), LABELS)\n",
    "        TEST_LOSSES.append(LOSS.item())\n",
    "# Plot the loss curve^\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(TEST_LOSSES, label='Test Loss')\n",
    "# Epoch of tesing^\n",
    "plt.xlabel('Epoch of testing')\n",
    "# Loss^\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(DATE_LABELS, PREDICTED_PRICES, label='Predicted Prices', color='blue')\n",
    "plt.plot(DATE_LABELS, ACTUAL_PRICES, label='Actual Prices', color='green')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Price')\n",
    "plt.title('Predicted vs Actual Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import torch\n",
    "#TEST_DATASET\n",
    "TEST_DATASET = StockPriceDataset(TESTING_DATA, TOKENIZER)\n",
    "#TEST_DATALOADER^\n",
    "TEST_DATALOADER = DataLoader(TEST_DATASET, batch_size=32, shuffle=False)\n",
    "\n",
    "#Switch to evaluation mode^\n",
    "MODEL.eval()\n",
    "\n",
    "#Store prediction^\n",
    "#Store actuals^\n",
    "PREDICTIONS = []\n",
    "ACTUALS = []\n",
    "with torch.no_grad():\n",
    "    #BATCH^\n",
    "    for BATCH in TEST_DATALOADER:\n",
    "        #INPUT_IDS^\n",
    "        INPUT_IDS = BATCH['input_ids']\n",
    "        #ATTENTION_MASK^\n",
    "        ATTENTION_MASK = BATCH['attention_mask']\n",
    "        #LABELS^\n",
    "        LABELS = BATCH['labels'].numpy() # Actual^\n",
    "        \n",
    "        #OUTPUT^\n",
    "        OUTPUT = MODEL(input_ids = INPUT_IDS, attention_mask = ATTENTION_MASK)\n",
    "        PREDICTION = OUTPUT.detach().numpy().flatten() # Prediction^\n",
    "        PREDICTIONS.extend(PREDICTION)\n",
    "        ACTUALS.extend(LABELS)\n",
    "#Calculate R-square value^\n",
    "r2 = r2_score(ACTUALS, PREDICTIONS)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
